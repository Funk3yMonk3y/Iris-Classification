{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Klassifikation\n\nIn diesem Beispiel versuchen wir ein tieferes Verständnis für Logistische Regression und Softmax Regression zu bekommen. Dazu plotten wir einige Entscheidungsgrenzen unserer Klassifikatoren.\n\nZuerst laden wir das Iris Datenset. Dieses Datenset beinhaltet 150 Beobachtungen von jeweils vier Attributen von Schwertlilien. Gemessen wurden dabei jeweils die Breite und die Länge des Kelchblatts (Sepalum) sowie des Kronblatts (Petalum) in Zentimeter. Des weiteren ist für jeden Datensatz die Art der Schwertlilie (Iris setosa, Iris virginica oder Iris versicolor) angegeben.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\ndata = load_iris(as_frame=True)\ndata.data.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:37:54.743806Z","iopub.execute_input":"2022-07-02T13:37:54.744266Z","iopub.status.idle":"2022-07-02T13:37:54.761988Z","shell.execute_reply.started":"2022-07-02T13:37:54.744238Z","shell.execute_reply":"2022-07-02T13:37:54.760779Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data.target_names","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:37:56.677858Z","iopub.execute_input":"2022-07-02T13:37:56.678147Z","iopub.status.idle":"2022-07-02T13:37:56.683796Z","shell.execute_reply.started":"2022-07-02T13:37:56.678107Z","shell.execute_reply":"2022-07-02T13:37:56.683061Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Binärer Klassifikator\n\nZu Visualisierungszwecken benutzen wir nur die Features Petal-Länge (Index 2) und Petal-Breite (Index 3). Weiters wollen wir zuerst einen Klassifikator trainieren, welcher Iris Virginica (Index 2) von den restlichen Iris Arten unterscheiden kann.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data.data.iloc[:, 2:]  # wähle nur `petal length` und `petal width`\ny = data.target == 2  # Binärer Vektor, 1: Iris Virginica, 0: nicht Iris Virginica\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:37:58.422522Z","iopub.execute_input":"2022-07-02T13:37:58.422825Z","iopub.status.idle":"2022-07-02T13:37:58.434276Z","shell.execute_reply.started":"2022-07-02T13:37:58.422794Z","shell.execute_reply":"2022-07-02T13:37:58.433392Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## 3a. Binäre Logistische Regression\n- Trainiere eine binäre Logistische Regression (`sklearn.linear_model.LogisticRegression`) um Iris Virginica zu erkennen.\n- Benutze eine l2 Regularisierung `penalty='l2'` und `GridSearchCV` für den Parameter `C`.\n- Welchen Einfluss hat der Hyperparameter C?","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nimport numpy as np\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:38:01.386074Z","iopub.execute_input":"2022-07-02T13:38:01.386327Z","iopub.status.idle":"2022-07-02T13:38:01.393903Z","shell.execute_reply.started":"2022-07-02T13:38:01.386302Z","shell.execute_reply":"2022-07-02T13:38:01.393189Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:28:53.210086Z","iopub.execute_input":"2022-07-02T13:28:53.210987Z","iopub.status.idle":"2022-07-02T13:28:53.215161Z","shell.execute_reply.started":"2022-07-02T13:28:53.210911Z","shell.execute_reply":"2022-07-02T13:28:53.214298Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"log_reg = LogisticRegression(penalty='l2')\nlog_reg.fit(X_train, y_train)\ncross_validate(log_reg, X, y, cv = 3)\nC = np.logspace(0, 4, 10)\nhyperparameters = dict(C=C)\n             \ngrid_search=GridSearchCV(log_reg,hyperparameters, cv = 5, verbose = 1, n_jobs= 1)\nbest_model=grid_search.fit(X_train, y_train)\nprint(best_model.best_estimator_.get_params()['C'])\nprint(\"The mean accuracy of the model is:\",best_model.score(X_train,y_train))","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:38:06.974499Z","iopub.execute_input":"2022-07-02T13:38:06.974831Z","iopub.status.idle":"2022-07-02T13:38:07.457666Z","shell.execute_reply.started":"2022-07-02T13:38:06.974801Z","shell.execute_reply":"2022-07-02T13:38:07.456677Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Im nächsten Schritt wollen wir die Entscheidungsgrenze plotten. Forme dazu die Klassenzugehörigkeitswahrscheinlichkeit in Form der Sigmoid Funktion auf $x_2$ (Petal-Breite) um und setze $p=0.5$.\n$$ p = \\sigma(x) = {1 \\over 1 + \\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)}$$\n\n- Schreibe eine Python Funktion (benutze die Parameter `intercept_` und `coef_`).\n- Erstelle einen Plot mit dem Datensatz (nur Petal Länge und Petal Breite) und zeichne die Entscheidungsgrenze ein.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nlog_reg_n = LogisticRegression()\nlog_reg_n.fit(X_train, y_train)\n\ndef decision_boundary(x):\n    b0 = log_reg_n.intercept_[0]\n    b1,b2 = log_reg_n.coef_[0]\n    x2 = -(b0+b1*x)/b2\n    return x2\n\nplt.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], c=y_train)\nplt.xlabel('petal length')\nplt.ylabel('petal width')\nx = np.arange(2,7,0.1)\ny = decision_boundary(x)\nplt.plot(x, y)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:38:09.570384Z","iopub.execute_input":"2022-07-02T13:38:09.571280Z","iopub.status.idle":"2022-07-02T13:38:09.773727Z","shell.execute_reply.started":"2022-07-02T13:38:09.571228Z","shell.execute_reply":"2022-07-02T13:38:09.772892Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## 3b. Entscheidungsgrenze von Iris Versicolor\n- Trainiere eine Logistische Regression für Iris Versicolor.\n- Plotte wie im vorherigen Beispiel die Entscheidungsgrenze.","metadata":{}},{"cell_type":"code","source":"X = data.data.iloc[:, 2:]  # wähle nur `petal length` und `petal width`\ny = data.target == 1  # Binärer Vektor 1: Iris Versicolor, 0: nicht Iris Versicolor\nX_train, X_test, y_train, y_test = train_test_split(X, y == 1, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:38:16.246324Z","iopub.execute_input":"2022-07-02T13:38:16.247211Z","iopub.status.idle":"2022-07-02T13:38:16.253986Z","shell.execute_reply.started":"2022-07-02T13:38:16.247172Z","shell.execute_reply":"2022-07-02T13:38:16.253026Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"log_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)\n\ndef decision_boundary(x):\n    b0 = log_reg.intercept_[0]\n    b1,b2 = log_reg.coef_[0]\n    x2 = -(b0+b1*x)/b2\n    return x2\n\nplt.scatter(X_train.iloc[:, 0], X_train.iloc[:, 1], c=y_train)\nplt.xlabel(\"petal length Iris Versicolor\")\nplt.ylabel(\"petal with Iris Versicolor\")\nx=np.arange(2,7,0.1)\ny=decision_boundary(x)\nplt.plot(x, y)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:38:18.766740Z","iopub.execute_input":"2022-07-02T13:38:18.767359Z","iopub.status.idle":"2022-07-02T13:38:19.063395Z","shell.execute_reply.started":"2022-07-02T13:38:18.767316Z","shell.execute_reply":"2022-07-02T13:38:19.062589Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## 3c. Softmax Regression\nUm einen Multioutput Klassifikator zu trainieren, können wir einfach den ursprünglichen Targetvektor verwenden. `sklearn` trainiert dann eine Softmax Regression.\n","metadata":{}},{"cell_type":"code","source":"X = data.data.iloc[:, 2:]  # wähle nur `petal length` und `petal width`\ny = data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n\nmodel = LogisticRegression(penalty='none', multi_class='multinomial')\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:29:21.252797Z","iopub.execute_input":"2022-07-02T13:29:21.253155Z","iopub.status.idle":"2022-07-02T13:29:21.280988Z","shell.execute_reply.started":"2022-07-02T13:29:21.253123Z","shell.execute_reply":"2022-07-02T13:29:21.280147Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Um die Entschdeidungsgrenzen zu plotten verwenden wir einen Konturplot.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(model, X, y):\n    plt.figure()\n    petal_length = np.arange(1, 7, 0.1)\n    petal_width = np.arange(0, 3, 0.1)\n\n    X_grid, Y_grid = np.meshgrid(petal_length, petal_width)\n\n    def pred(x, y, label):\n        return model.predict_proba(np.c_[x, y])[0, label]\n\n    pred = np.frompyfunc(pred, 3, 1, )\n\n    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y)\n\n    Z = pred(X_grid, Y_grid, 0)\n    plt.contour(X_grid, Y_grid, Z, [0.5])\n    Z = pred(X_grid, Y_grid, 1)\n    plt.contour(X_grid, Y_grid, Z, [0.5])\n    Z = pred(X_grid, Y_grid, 2)\n    plt.contour(X_grid, Y_grid, Z, [0.5])\n    \nplot_decision_boundary(model, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:40:14.311506Z","iopub.execute_input":"2022-07-02T13:40:14.312037Z","iopub.status.idle":"2022-07-02T13:40:15.209426Z","shell.execute_reply.started":"2022-07-02T13:40:14.311991Z","shell.execute_reply":"2022-07-02T13:40:15.208511Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"- Plotte die Entscheidungsgrenzen für verschiedene Werte des Hyperparameters `C`. Verwende dazu `penalty='l2'`.\n- Berechne die Wahrheitsmatrix (Confusion Matrix)\n- Berechne folgenden Werte: `accuracy_score, precision_score, recall_score, f1_score`","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train,y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:41:41.465607Z","iopub.execute_input":"2022-07-02T13:41:41.466127Z","iopub.status.idle":"2022-07-02T13:41:41.481876Z","shell.execute_reply.started":"2022-07-02T13:41:41.466096Z","shell.execute_reply":"2022-07-02T13:41:41.480948Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","metadata":{"execution":{"iopub.status.busy":"2022-07-02T13:30:09.757103Z","iopub.execute_input":"2022-07-02T13:30:09.757396Z","iopub.status.idle":"2022-07-02T13:30:09.761505Z","shell.execute_reply.started":"2022-07-02T13:30:09.757368Z","shell.execute_reply":"2022-07-02T13:30:09.760895Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}